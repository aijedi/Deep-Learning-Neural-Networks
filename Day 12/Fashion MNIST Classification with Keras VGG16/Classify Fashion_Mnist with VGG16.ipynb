{"cells":[{"metadata":{"_cell_guid":"0fd216d4-dd9f-4c8f-9292-df9aa7c17aca","_uuid":"1c70132a25ea771c9caaa6b9dbfd2d65955be21d"},"cell_type":"markdown","source":"**Experimentation on Fashion Mnist with VGG16**\nTo demonstrate\n1) Converting images with 1 channel to 3 channels\n2) Resizing the images\n3) Using VGG16 base model, appending with other layers and extracting features\n4) Reduce learning, early stopping in callback methods"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, time\nimport matplotlib.pyplot as plt\n#from keras.datasets import fashion_mnist\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Dropout, Flatten\n#from keras.layers.advanced_activations import LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG16;\nfrom keras.applications.vgg16 import preprocess_input\nimport os","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read csv data files\ntrain_data = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\ntest_data = pd.read_csv('../input/fashionmnist/fashion-mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7e89d01e-7dcb-4679-8682-f0901f7acbeb","_uuid":"67003184dbfdac55a3cb8633c25bd38edafd6276","trusted":true},"cell_type":"code","source":"train_data.shape #(60,000*785)\ntest_data.shape #(10000,785)\ntrain_X= np.array(train_data.iloc[:,1:])\ntest_X= np.array(test_data.iloc[:,1:])\ntrain_Y= np.array (train_data.iloc[:,0]) # (60000,)\ntest_Y = np.array(test_data.iloc[:,0]) #(10000,)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"108ad2b4-7aa5-4bd1-bdef-f6e3ceed9f31","_uuid":"25e392915c2cdb4b7fef6199c2747db6bfa7ab2d","trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a736169bc4b6a1d172660b4cc390d98d7c24ac0a","_cell_guid":"657bf656-6266-4ff7-9610-a8f4e85b3e29","trusted":true},"cell_type":"code","source":"train_X.shape, test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1878c711627e46e0dc24cc9d1c886d87f2481bd","_cell_guid":"90e0b7cd-4941-45e6-9cc5-35966a649a7a","trusted":true},"cell_type":"code","source":"classes = np.unique(train_Y)\nnum_classes = len(classes)\nnum_classes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"65f644bd-0d53-4f93-9d56-44e5ee362124","_uuid":"36b3aa039739b9465903682b38a911cb5550ebd7","trusted":true},"cell_type":"code","source":"# Convert the images into 3 channels\ntrain_X=np.dstack([train_X] * 3)\ntest_X=np.dstack([test_X]*3)\ntrain_X.shape,test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"881b27ff-55a1-47f5-b814-e6e269b0c841","_uuid":"18a5d76e99531a6807c561ee8e4d75f0d726d78f","trusted":true},"cell_type":"code","source":"# Reshape images as per the tensor format required by tensorflow\ntrain_X = train_X.reshape(-1, 28,28,3)\ntest_X= test_X.reshape (-1,28,28,3)\ntrain_X.shape,test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fe00caac550a34af0076645c5fa0fa64b2908cc","_cell_guid":"088dd0e5-b28c-4d44-b7f1-ad972ec93f8b","trusted":true},"cell_type":"code","source":"# Resize the images 48*48 as required by VGG16\nfrom keras.preprocessing.image import img_to_array, array_to_img\ntrain_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in train_X])\ntest_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in test_X])\n#train_x = preprocess_input(x)\ntrain_X.shape, test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7bf156c69f540216d0e7b5de3e7b0ad50592099d","_cell_guid":"b7f7e12c-01ce-47e5-95bd-069cb89c9258","trusted":true},"cell_type":"code","source":"# Normalise the data and change data type\ntrain_X = train_X / 255.\ntest_X = test_X / 255.\ntrain_X = train_X.astype('float32')\ntest_X = test_X.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6a11e96346d7ff14c5614375ab9237809f4750fd","_cell_guid":"f2632a1e-c62b-43ca-8417-3dd16176d6a3","trusted":true},"cell_type":"code","source":"# Converting Labels to one hot encoded format\ntrain_Y_one_hot = to_categorical(train_Y)\ntest_Y_one_hot = to_categorical(test_Y)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e83f7fb410d318f63141efd5318c4b7ea06c925a","_cell_guid":"854ef9e9-4fac-4021-8742-258bbc64f7d7","trusted":true},"cell_type":"code","source":"# Splitting train data as train and validation data\ntrain_X,valid_X,train_label,valid_label = train_test_split(train_X,\n                                                           train_Y_one_hot,\n                                                           test_size=0.2,\n                                                           random_state=13\n                                                           )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30c531675abd917e1aff9c7174000a8da61c8e9a","_cell_guid":"533aef5c-ea10-46f8-8678-ae8466c61358","trusted":true},"cell_type":"code","source":"# Finally check the data size whether it is as per tensorflow and VGG16 requirement\ntrain_X.shape,valid_X.shape,train_label.shape,valid_label.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"113ac9912ec68548ab6c9663d2a268b341d19318","_cell_guid":"b9a862b8-83a7-4920-9662-4e7d5b06fa2a","trusted":true},"cell_type":"code","source":"# Define the parameters for instanitaing VGG16 model. \nIMG_WIDTH = 48\nIMG_HEIGHT = 48\nIMG_DEPTH = 3\nBATCH_SIZE = 16","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1efa26a809251bb45f142337617313b8a8b0257c","_cell_guid":"502df777-d9c5-4a15-9928-c40930a786a7","trusted":true,"collapsed":true},"cell_type":"code","source":"# Preprocessing the input \ntrain_X = preprocess_input(train_X)\nvalid_X = preprocess_input(valid_X)\ntest_X  = preprocess_input (test_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b3f38f3d269f4ee886e7617f9aafe6b2dae6896","_cell_guid":"229322c0-79c7-499c-b8f5-7eafd8fd45da","trusted":true},"cell_type":"code","source":"#  Create base model of VGG16\nconv_base = VGG16(weights='../input/keras-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                  include_top=False, \n                  input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n                 )\nconv_base.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aaa03ae6-b327-4155-bb3f-bc276cfc5f79","_uuid":"3303dec8034ae9001f6bbfd172772c3763c0ca95","trusted":true},"cell_type":"code","source":"# Extracting features\ntrain_features = conv_base.predict(np.array(train_X), batch_size=BATCH_SIZE, verbose=1)\ntest_features = conv_base.predict(np.array(test_X), batch_size=BATCH_SIZE, verbose=1)\nval_features = conv_base.predict(np.array(valid_X), batch_size=BATCH_SIZE, verbose=1)\n#for layer in conv_base.layers:\n#    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"edd015a505d1088b0aacc5f7ee5434cfa40929ef","_cell_guid":"ca1c5a1a-c870-4c5a-a086-77aaa1934f28","trusted":true},"cell_type":"code","source":"# 6.1 Saving the features so that they can be used for future\nnp.savez(\"train_features\", train_features, train_label)\nnp.savez(\"test_features\", test_features, test_Y)\nnp.savez(\"val_features\", val_features, valid_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"06ee60c6-d892-47c9-a29e-498a9a8e6888","_uuid":"afc55e2793e4c9c14330bbe3d2b17c98ff680843","trusted":true},"cell_type":"code","source":"# Current shape of features\nprint(train_features.shape, \"\\n\",  test_features.shape, \"\\n\", val_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e8e7f72c0d7a6702c8e68b68ad050d81f052e14d","_cell_guid":"da226aa2-e159-4426-9e11-624b109599ae","trusted":true},"cell_type":"code","source":"# Flatten extracted features\ntrain_features_flat = np.reshape(train_features, (48000, 1*1*512))\ntest_features_flat = np.reshape(test_features, (10000, 1*1*512))\nval_features_flat = np.reshape(val_features, (12000, 1*1*512))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3fae918d3a48e24cbf3de5190f5c0616d13d696c"},"cell_type":"code","source":"from keras import models\nfrom keras.models import Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import callbacks\nfrom keras.layers.advanced_activations import LeakyReLU","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4ca32f8-2a43-453c-838e-efcb89b21df2","_uuid":"4bf35298843795507b757fbcbe2b056f2f51e4b8","trusted":true},"cell_type":"code","source":"# 7.0 Define the densely connected classifier followed by leakyrelu layer and finally dense layer for the number of classes\nNB_TRAIN_SAMPLES = train_features_flat.shape[0]\nNB_VALIDATION_SAMPLES = val_features_flat.shape[0]\nNB_EPOCHS = 100\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(512, activation='relu', input_dim=(1*1*512)))\nmodel.add(layers.LeakyReLU(alpha=0.1))\nmodel.add(layers.Dense(num_classes, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c22c87623f160988f1aa302bfe76c79c1c08aaba","_cell_guid":"c9c315be-b7c7-484d-9add-0a3aeea4cd14","trusted":true},"cell_type":"code","source":"# Compile the model.\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=optimizers.Adam(),\n  # optimizer=optimizers.RMSprop(lr=2e-5),\n    metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77267b8d-0cb3-47df-8aeb-abfc715cdccb","_uuid":"620794510507d763cbdbec85cf88bc74a38c689c","trusted":true},"cell_type":"code","source":"# Incorporating reduced learning and early stopping for callback\nreduce_learning = callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=2,\n    verbose=1,\n    mode='auto',\n    epsilon=0.0001,\n    cooldown=2,\n    min_lr=0)\n\neary_stopping = callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=7,\n    verbose=1,\n    mode='auto')\n\ncallbacks = [reduce_learning, eary_stopping]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa421c86f8429c230970142425f236a0e6affb1b","_cell_guid":"111c53bc-3f72-426e-abfb-a902d8aa1933","trusted":true},"cell_type":"code","source":"# Train the the model\nhistory = model.fit(\n    train_features_flat,\n    train_label,\n    epochs=NB_EPOCHS,\n    validation_data=(val_features_flat, valid_label),\n    callbacks=callbacks\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9eb383670f32c4c8a4cc5ca3d4b5e8174f924e8","_cell_guid":"098cc9db-404e-4125-9b0a-3b30875cc79a","trusted":true},"cell_type":"code","source":"# plot the loss and accuracy\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9cde6c7a-22ef-4c40-a1f9-c23aa3ccbb87","_uuid":"806070772ca7bb192301cdc01dc60bb5b378c59a"},"cell_type":"markdown","source":"References\nhttps://www.kaggle.com/crawford/diagnose-lung-disease-with-vgg16\nhttps://www.programcreek.com/python/example/92213/keras.applications.vgg16.VGG16\nhttp://www.socouldanyone.com/2013/03/converting-grayscale-to-rgb-with-numpy.html"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}